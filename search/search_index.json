{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computa\u00e7\u00e3o em Nuvem","text":"Edi\u00e7\u00e3o <p>2024.2: roteiros </p> <p>2025.1: projeto</p>"},{"location":"#kit-u","title":"KIT-U","text":"<ul> <li>Giovana Cassoni Andrade</li> <li>Lucas Hix</li> </ul>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 26/03/2025</li> <li> Roteiro 2 - Data 26/03/2025</li> <li> Roteiro 3 - Data 28/05/2025</li> <li> Roteiro 4 - Data 28/05/2025</li> <li> Projeto - Data 28/05/2025</li> </ul>"},{"location":"#textos-e-imagens-dos-roteiros","title":"Textos e imagens dos roteiros","text":"<p>Para ver os roteiros originais em PDF, eles est\u00e3o presentes em suas respectivas abas de roteiro.</p> <p>Para obter os textos, legendas e imagens dos roteiros para coloc\u00e1-los nas p\u00e1ginas, foram instaladas a biblioteca poppler e a ferramenta pdfminer.six com:</p> <pre><code>brew install poppler\npip install pdfminer.six\n</code></pre> <p>E, para cada um dos roteiros, foram utilizados os comandos a seguir:</p> <pre><code>pdfimages -png Roteiro_X_de_Cloud.pdf imgs/roteiroX\npdf2txt.py Roteiro_X_de_Cloud.pdf &gt; Roteiro_X_de_Cloud.md\n</code></pre>"},{"location":"projeto/documentacao/","title":"Documenta\u00e7\u00e3o","text":""},{"location":"projeto/documentacao/#objetivo","title":"Objetivo","text":""},{"location":"projeto/documentacao/#projeto","title":"Projeto","text":"<p>Link do Reposit\u00f3rio: https://github.com/Peng1104/Projeto-Cloud</p>"},{"location":"projeto/enunciado/","title":"Enunciado","text":"<p>Info</p> <p>PROJETO: DUPLA</p> <p>DEADLINE: 28.mai.2025</p> <p>Enunciado original</p> <p>O enunciado completo do projeto se encontra na p\u00e1gina da disciplina no link: https://insper.github.io/computacao-nuvem/projetos_2025-1/projeto/</p> <p>O projeto trata de uma API RESTful que deve ser capaz de cadastrar e autenticar usu\u00e1rios, al\u00e9m de permitir a consulta de dados de terceiros. Ap\u00f3s a constru\u00e7\u00e3o da API, o projeto deve ser dockerizado e, ent\u00e3o, implantado na AWS.</p>"},{"location":"projeto/enunciado/#etapa-1","title":"Etapa 1","text":""},{"location":"projeto/enunciado/#construcao-da-api","title":"Constru\u00e7\u00e3o da API","text":"<p>A API dever ter no m\u00ednimo 3 endpoints:</p> Registro de Usu\u00e1rio Endpoint<pre><code>POST /registrar\n</code></pre> <p>Request JSON payload<pre><code>{\n    \"nome\": \"Disciplina Cloud\",\n    \"email\": \"cloud@insper.edu.br\",\n    \"senha\": \"cloud0\"\n}\n</code></pre></p> <p>Response JSON payload<pre><code>{\n    \"jwt\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkRpc2NpcGxpbmEgQ2xvdWQiLCJpYXQiOjE1MTYyMzkwMjJ9.s76o9X4UIANSI-aTF8UhqnBYyIRWw_WH4ut8Xqmo6i0\"\n}\n</code></pre></p> Autentica\u00e7\u00e3o de Usu\u00e1rio Endpoint<pre><code>POST /login\n</code></pre> <p>Request JSON payload<pre><code>{\n    \"email\": \"cloud@insper.edu.br\",\n    \"senha\": \"cloud0\"\n}\n</code></pre></p> <p>Response JSON payload<pre><code>{\n    \"jwt\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.\n            eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkRpc2N\n            pcGxpbmEgQ2xvdWQiLCJpYXQiOjE1MTYyMzkwMjJ9.\n            s76o9X4UIANSI-aTF8UhqnBYyIRWw_WH4ut8Xqmo6i0\"\n}\n</code></pre></p> Aquisi\u00e7\u00e3o dos Dados Endpoint<pre><code>GET /consultar\n\n### HEADER\nAuthorization: Bearer &lt;JWT&gt;\n</code></pre> <ul> <li> <p>Response: A resposta pode ser qualquer scrap de uma p\u00e1gina de terceiros, e o formato tamb\u00e9m pode ser qualquer um.</p> </li> <li> <p>Os dados devolvidos pela consulta devem ser de uma p\u00e1gina de terceiros, e devem ser atualizados frequentemente.</p> </li> <li> <p>Caso o usu\u00e1rio n\u00e3o tenha um token v\u00e1lido, a API deve retornar um erro 403.</p> </li> </ul>"},{"location":"projeto/enunciado/#dockerinzing","title":"Dockerinzing","text":"<p>Quando o c\u00f3digo da API estiver pronto, ele deve ser dockerizado. Para isso, deve-se criar um arquivo <code>Dockerfile</code> e um <code>compose.yaml</code> para a execu\u00e7\u00e3o da aplica\u00e7\u00e3o.</p> <p>O docker compose deve conter pelo menos 2 servi\u00e7os: a aplica\u00e7\u00e3o e o banco de dados. A aplica\u00e7\u00e3o deve ser capaz de se conectar ao banco de dados e realizar as opera\u00e7\u00f5es de CRUD.</p> <p>A aplica\u00e7\u00e3o deve ser autocontida, ou seja, deve ser poss\u00edvel executar a aplica\u00e7\u00e3o apenas com o comando <code>docker compose up</code> - pois isso \u00e9 parte essencial da entrega.</p>"},{"location":"projeto/enunciado/#publicacao-no-docker-hub","title":"Publica\u00e7\u00e3o no Docker Hub","text":"<p>Ap\u00f3s a dockeriza\u00e7\u00e3o, o projeto deve ser publicado no Docker Hub. O link do Docker Hub deve ser inclu\u00eddo na documenta\u00e7\u00e3o do projeto.</p> <p>A publica\u00e7\u00e3o no docker hub deve ser feita via linha de comando. E os comandos utilizados devem ser inclu\u00eddos na documenta\u00e7\u00e3o do projeto.</p> Vari\u00e1veis de Ambiente <p>As credenciais do banco de dados e JWT devem ser passadas via vari\u00e1veis de ambiente, por um arquivo <code>.env</code>. Todavia, PARA FACILITAR A CORRE\u00c7\u00c3O, as credenciais podem ser passadas diretamente no <code>compose.yaml</code> por valores padr\u00f5es, para que n\u00e3o tenha que haver um arquivo de vari\u00e1veis de ambiente. Exemplo:</p> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol> .env<pre><code>POSTGRES_DB=meuprojeto\nPOSTGRES_USER=meuprojeto\nPOSTGRES_PASSWORD=S3cr3t\n</code></pre> <p>Ao executar, o docker compose ir\u00e1 utilizar as vari\u00e1veis de ambiente do arquivo <code>.env</code>, caso existam, sen\u00e3o, utilizar\u00e1 os valores padr\u00f5es definidos j\u00e1 dentro do arquivo <code>compose.yaml</code>.</p>"},{"location":"projeto/enunciado/#etapa-2","title":"Etapa 2","text":""},{"location":"projeto/enunciado/#aws","title":"AWS","text":"<p>O pr\u00f3ximo passo \u00e9 implantar a aplica\u00e7\u00e3o na AWS.</p>"},{"location":"projeto/enunciado/#entregas","title":"Entregas","text":"Entrega Etapa 1 <p>A entrega dever\u00e1 ser um link do projeto no GitHub, contendo o c\u00f3digo da API e o Dockerfile.</p> <p>Deve haver uma documenta\u00e7\u00e3o b\u00e1sica do projeto no MkDocs, contendo:</p> <ul> <li>explica\u00e7\u00e3o do projeto - scrap do que foi feito;</li> <li>explica\u00e7\u00e3o de como executar a aplica\u00e7\u00e3o;</li> <li>documenta\u00e7\u00e3o dos endpoints da API;</li> <li>screenshot com os endpoints testados;</li> <li>video de execu\u00e7\u00e3o da aplica\u00e7\u00e3o - de at\u00e9 1 minuto;</li> <li>link para o docker hub do projeto;</li> <li>refer\u00eancia expl\u00edcita a localiza\u00e7\u00e3o do arquivo <code>compose.yaml</code>;</li> <li>o arquivo <code>compose.yaml</code> FINAL (entregue) deve utilizar apenas images do docker hub (inclusive as geradas para a api), ou seja, n\u00e3o deve ter <code>build</code> dentro dele.</li> </ul> Entrega Etapa 2 <p>A entrega dever\u00e1 ser um link do projeto no GitHub, o mesmo do anterior, mas para uma sess\u00e3o sobre a publica\u00e7\u00e3o na AWS, contendo o uma breve explica\u00e7\u00e3o e um link para um v\u00eddeo, explicando e executando o trabalho entregue.</p> <p>O v\u00eddeo apresentado deve ter entre 2 e 3 minutos e DEVE demonstrar TODOS os seguintes itens:</p> <ul> <li>logar na conta e acessar o projeto;</li> <li>explicar o que foi feito e mostrar os componentes do projeto (eks, roles, etc);</li> <li>executar o comando <code>kubectl get pods</code> e mostrar os pods rodando;   <pre><code>kubectl get pods\n</code></pre></li> <li>mostrar o projeto executando na AWS: chamada da API por um cliente (curl, postman, etc);</li> </ul> <p>No texto deve haver um link para os arquivos de configura\u00e7\u00e3o do Kubernetes (arquivos .yaml: deployment.yaml, service.yaml, etcs), reposit\u00f3rio do projeto.</p>"},{"location":"projeto/enunciado/#rubrica","title":"Rubrica","text":"Rubrica Etapa Crit\u00e9rio Nota Observa\u00e7\u00f5es 1 API + Dockeriza\u00e7\u00e3o + Docker Hub + Documenta\u00e7\u00e3o C 2 AWS + 1 conceito - 2 conceitos se n\u00e3o entregar a etapa do AWS AWS + Documenta\u00e7\u00e3o + 2 conceitos"},{"location":"projeto/enunciado/#docker-compose","title":"Docker compose","text":""},{"location":"projeto/enunciado/#material-de-aula","title":"Material de aula","text":"estrutura para dois containers num mesmo compose<pre><code>api\n  Dockerfile\nweb\n  Dockerfile\n  hello.txt\n.env\ncompose.yaml\n</code></pre>"},{"location":"projeto/enunciado/#comandos","title":"Comandos","text":"<ul> <li> <p>Executando o docker compose: <pre><code>docker compose up -d --build\n</code></pre></p> </li> <li> <p>Parando o docker compose: <pre><code>docker compose down\n</code></pre></p> </li> </ul>"},{"location":"roteiros/roteiro1/main/","title":"Roteiro","text":""},{"location":"roteiros/roteiro1/main/#objetivo","title":"Objetivo","text":"<p>O objetivo principal desse roteiro \u00e9:</p> <ul> <li>entender os conceitos b\u00e1sicos sobre uma plataforma de gerenciamento de hardware</li> <li>introduzir conceitos b\u00e1sicos sobre redes de computadores</li> </ul>"},{"location":"roteiros/roteiro1/main/#roteiro","title":"Roteiro","text":"<p>Para visualizar o arquivo PDF feito e entregue, consultar o link a seguir: Roteiro 1 - PDF.</p>"},{"location":"roteiros/roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Warning</p> <p>O formato e a quantidade de tarefas do roteiro feito (em 2024.2) \u00e9 diferente do enunciado (de 2025.1), por\u00e9m coment\u00e1rios s\u00e3o feitos para melhor compreens\u00e3o e \u00e9 majoritariamente seguido o mesmo conte\u00fado.</p>"},{"location":"roteiros/roteiro1/main/#infra","title":"Infra","text":"<p>Para criar a infraestrutura, foram instalados o Ubuntu e o MaaS, realizando todas as configura\u00e7\u00f5es necess\u00e1rias.</p>"},{"location":"roteiros/roteiro1/main/#parte-i","title":"Parte I","text":"<p>Deploy do Ubuntu, instalar a aplica\u00e7\u00e3o PostgreSQL e criar uma base de dados.</p> Tarefa-1 <p>Estude os comandos ping, ifconfig, systemctl, telnet, ufw, curl, wget e journalctl. Com estes comandos apresente prints das Telas  que provam que o banco de dados est\u00e1:</p> <pre><code>1. Funcionando e seu Status est\u00e1 como \"Ativo\" para o Sistema Operacional\n2. Acessivel na pr\u00f3pria maquina na qual ele foi implantado.\n3. Acessivel a partir de uma conex\u00e3o vinda da m\u00e1quina MAIN.\n4. Em qual porta este servi\u00e7o est\u00e1 funcionando.\n</code></pre> <p>No roteiro que fizemos essas telas n\u00e3o foram requeridas.</p>"},{"location":"roteiros/roteiro1/main/#parte-ii","title":"Parte II","text":"<p>Subir uma aplica\u00e7\u00e3o Django. </p> Tarefa-2 <p>De um print das Telas abaixo:</p> <pre><code>1. Do Dashboard do **MAAS** com as m\u00e1quinas.\n2. Da aba images, com as imagens sincronizadas.\n3. Da Aba de cada maquina(5x) mostrando os testes de hardware e commissioning com Status \"OK\"\n</code></pre> <p></p> <p>Figura 1 - Painel Django acessado via tunelamento </p> <p></p> <p>Figura 2 - Dashboard do MAAS com as m\u00e1quinas </p> <p></p> <p>Figura 3 - Aba imagens, com as imagens sincronizadas</p> <p></p> <p>Figura 4 - Server 1: Network</p> <p></p> <p>Figura 5 - Server 1: Commissioning </p> <p></p> <p>Figura 6 - Server 1: Testes </p> <p></p> <p>Figura 7 - Server 2: Network </p> <p></p> <p>Figura 8 - Server 2: Commissioning</p> <p></p> <p>Figura 9 - Server 2: Testes </p> <p></p> <p>Figura 10 - Server 3: Network</p> <p></p> <p>Figura 11 - Server 3: Commissioning</p> <p></p> <p>Figura 12 - Server 3: Tests</p> <p></p> <p>Figura 13 - Server 4: Network </p> <p></p> <p>Figura 14 - Server 4: Commissioning </p> <p></p> <p>Figura 15 - Server 4: Tests  </p> <p></p> <p>Figura 16 - Server 5: Network </p> <p></p> <p>Figura 17 - Server 5: Comissioning  </p> <p></p> <p>Figura 18 - Server 5: Tests </p> <p>Utilizando o Ansible - deploy automatizado de aplica\u00e7\u00e3o.</p> Tarefa-3 <ol> <li>De um print da tela do Dashboard do MAAS com as 2 Maquinas e seus respectivos IPs.</li> <li>De um print da aplicacao Django, provando que voce est\u00e1 conectado ao server </li> <li>Explique como foi feita a implementacao manual da aplicacao Django e banco de dados.</li> </ol> <p>No roteiro que fizemos, ESSA TAREFA FOI PEDIDA NO ROTEIRO 2, mas colocamos as respostas abaixo: </p> <p></p> <p>Figura 19 - Dashboard do MAAS com as 2 m\u00e1quinas </p> <p></p> <p>Figura 20 - Painel Django acessado via tunelamento </p> <p>Explique como foi feita a implementa\u00e7\u00e3o manual da aplica\u00e7\u00e3o Django e banco de dados.</p> <p>Fazer toda a conex\u00e3o com o MAAS e logar no MAAS, para poder conectar \u00e0 m\u00e1quina e realizar seu deploy. Depois, fazer git clone e install para deixar o ambiente django pronto para o seu uso. </p> <p>Subindo uma aplica\u00e7\u00e3o Django utilizando o Ansible.</p> Tarefa-4 <p>Teste o acesso, caso esteja tudo certo, fa\u00e7a a tarefa abaixo</p> <pre><code>1. De um print da tela do Dashboard do MAAS com as 3 Maquinas e seus respectivos IPs.\n2. De um print da aplicacao Django, provando que voce est\u00e1 conectado ao server2 \n3. De um print da aplicacao Django, provando que voce est\u00e1 conectado ao server3 \n4. Explique qual diferenca entre instalar manualmente a aplicacao Django e utilizando o Ansible.\n</code></pre> <p>No roteiro que fizemos, ESSA TAREFA FOI PEDIDA NO ROTEIRO 2, mas colocamos as respostas abaixo:</p> <p></p> <p>Figura 21 - Dashboard do MAAS com as 3 m\u00e1quinas </p> <p></p> <p>Figura 22 - Aplica\u00e7\u00e3o Django provando que est\u00e1 conectado ao Server 2 </p> <p></p> <p>Figura 23 - Aplica\u00e7\u00e3o Django provando que est\u00e1 conectado ao Server 3 </p> <p>Explique  qual  a  diferen\u00e7a  entre  instalar  manualmente  a  aplica\u00e7\u00e3o  Django  e utilizando o Ansible.</p> <p>A diferen\u00e7a \u00e9 que, com o deploy feito no server, pode-se usar o Ansible para preparar o ambiente para a aplica\u00e7\u00e3o do django.</p> <p>Balanceamento de carga usando Proxy Reverso, \u00e9 instalando o nginx.</p> Tarefa-5 <p>Teste o acesso, caso esteja tudo certo, fa\u00e7a a tarefa abaixo:</p> <pre><code>1. De um print da tela do Dashboard do MAAS com as 4 Maquinas e seus respectivos IPs.\n2. Altere o conte\u00fado da mensagem contida na fun\u00e7\u00e3o `index` do arquivo `tasks/views.py` de cada server para distinguir ambos os servers. \n3. Fa\u00e7a um `GET request` para o path que voce criou em urls.py para o Nginx e tire 2 prints das respostas de cada request, provando que voce est\u00e1 conectado ao server 4, que \u00e9 o Proxy Reverso e que ele bate cada vez em um server diferente server2 e server3.\n</code></pre> <p>No roteiro que fizemos, ESSA TAREFA FOI PEDIDA NO ROTEIRO 2, mas colocamos as respostas abaixo:</p> <p></p> <p>Figura 24 - Dashboard do MAAS com as 4 m\u00e1quinas</p> <p></p> <p>Figura 25 - Resposta do request conectado ao Server2 </p> <p></p> <p>Figura 26 - Resposta do request conectado ao Server3 </p> <p>Agora finalizando, \u00e9 feito um release de todos os n\u00f3s no kit.</p>"},{"location":"roteiros/roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Este roteiro aborda o provisionamento e gerenciamento de infraestrutura Bare Metal com MaaS, permitindo tratar servidores f\u00edsicos como uma nuvem privada. A instala\u00e7\u00e3o do PostgreSQL garante maior controle sobre o desempenho e seguran\u00e7a do banco de dados, essencial para aplica\u00e7\u00f5es de grande porte.</p> <p>O uso do Ansible automatiza o deploy da aplica\u00e7\u00e3o Django, refor\u00e7ando a import\u00e2ncia da infraestrutura como c\u00f3digo (IaC) para padroniza\u00e7\u00e3o e efici\u00eancia operacional. Al\u00e9m disso, a implementa\u00e7\u00e3o do Nginx como proxy reverso melhora a escalabilidade e a disponibilidade da aplica\u00e7\u00e3o, distribuindo a carga de forma eficiente.</p> <p>O aprendizado adquirido refor\u00e7a pr\u00e1ticas essenciais de DevOps, automa\u00e7\u00e3o e escalabilidade para aplica\u00e7\u00f5es modernas.</p>"},{"location":"roteiros/roteiro2/main/","title":"Roteiro","text":""},{"location":"roteiros/roteiro2/main/#objetivo","title":"Objetivo","text":"<p>O objetivo principal desse roteiro \u00e9:</p> <ul> <li>entender os conceitos b\u00e1sicos sobre uma plataforma de gerenciamento de aplica\u00e7\u00f5es distribu\u00eddas</li> <li>entender os conceitos b\u00e1sicos de comunica\u00e7\u00e3o entre aplica\u00e7\u00f5es e servi\u00e7os</li> </ul>"},{"location":"roteiros/roteiro2/main/#roteiro","title":"Roteiro","text":"<p>Para visualizar o arquivo PDF feito e entregue, consultar o link a seguir: Roteiro 2 - PDF.</p>"},{"location":"roteiros/roteiro2/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Warning</p> <p>O formato e a quantidade de tarefas do roteiro feito (em 2024.2) \u00e9 diferente do enunciado (de 2025.1), por\u00e9m coment\u00e1rios s\u00e3o feitos para melhor compreens\u00e3o e \u00e9 majoritariamente seguido o mesmo conte\u00fado.</p>"},{"location":"roteiros/roteiro2/main/#infra","title":"Infra","text":"<p>Para criar a infraestrutura, foi instalado o Juju, realizando todas as configura\u00e7\u00f5es necess\u00e1rias.</p>"},{"location":"roteiros/roteiro2/main/#deployment-orchestration","title":"Deployment Orchestration","text":"<p>Instalados o Dashboard do Juju, o Grafana e o Prometheus, e \u00e9 feito o deploy das aplica\u00e7\u00f5es Grafana e Prometheus com o auxilio do Juju. Por fim, integra-se o Grafana com o Prometheus.</p> Tarefa-1 <ol> <li>De um print da tela do Dashboard do MAAS com as Maquinas e seus respectivos IPs.</li> <li>De um print de tela do comando \"juju status\" depois que o Grafana estiver \"active\". </li> <li>De um print da tela do Dashboard do Grafana com o Prometheus aparecendo como source.</li> <li>Prove (print) que voc\u00ea est\u00e1 conseguindo acessar o Dashboard a partir da rede do Insper.</li> <li>De um print na tela que mostra as aplica\u00e7\u00f5es sendo gerenciadas pelo JUJU (http://IP-Servi\u00e7o:8080/models/admin/maas)</li> </ol> <p>No roteiro que fizemos as telas dos pontos 4 e 5 n\u00e3o foram requeridas.</p> <p></p> <p>Figura 1 - Dashboard do MAAS com as m\u00e1quinas e seus respectivos IPs </p> <p></p> <p>Figura 2 - Comando \u201cjuju status\u201d com o Grafana ativo </p> <p></p> <p>Figura 3 - Dashboard do Grafana com o Prometheus aparecendo como source </p> <p></p> <p>Figura 4 - Comando do tunelamento com o Grafana </p>"},{"location":"roteiros/roteiro2/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Este roteiro demonstra o uso do Juju para orquestra\u00e7\u00e3o de deployment em infraestrutura Bare Metal, simplificando a configura\u00e7\u00e3o e gerenciamento de servi\u00e7os. A instala\u00e7\u00e3o do Juju Dashboard facilita a administra\u00e7\u00e3o visual da infraestrutura, melhorando a usabilidade e monitoramento.</p> <p>A utiliza\u00e7\u00e3o do Juju para implantar o Grafana e o Prometheus destaca a automa\u00e7\u00e3o no gerenciamento de aplica\u00e7\u00f5es, reduzindo a complexidade do deploy e garantindo consist\u00eancia na configura\u00e7\u00e3o. A integra\u00e7\u00e3o entre essas ferramentas permite a coleta e visualiza\u00e7\u00e3o de m\u00e9tricas em tempo real, essencial para a observabilidade e monitoramento eficaz da infraestrutura.</p>"},{"location":"roteiros/roteiro3/main/","title":"Roteiro","text":""},{"location":"roteiros/roteiro3/main/#objetivo","title":"Objetivo","text":"<p>O objetivo principal desse roteiro \u00e9:</p> <ul> <li>entender os conceitos b\u00e1sicos de Private Cloud</li> <li>aprofundar conceitos sobre redes virtuais SDN</li> </ul>"},{"location":"roteiros/roteiro3/main/#roteiro","title":"Roteiro","text":"<p>Para visualizar o arquivo PDF feito e entregue, consultar o link a seguir: Roteiro 3 - PDF.</p>"},{"location":"roteiros/roteiro3/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>O OpenStack \u00e9 um conjunto de componentes de software que oferecem servi\u00e7os comuns para a infraestrutura de cloud.</p> <p></p> <p>Figura 1 - OpenStack. Fonte: https://www.openstack.org/</p>"},{"location":"roteiros/roteiro3/main/#infra-nuvem-vm-servidor-virtual-privado-vps","title":"Infra (Nuvem VM) - Servidor Virtual Privado (VPS)","text":"<p>O OpenStack permitir\u00e1 distribuir virtual machines usando os n\u00f3s dispon\u00edveis no kit, mas antes de iniciar a instala\u00e7\u00e3o, deve-se verificar se o MAAS est\u00e1 configurado corretamente.</p> <p>A documenta\u00e7\u00e3o oficial do OpenStack est\u00e1 presente no link a seguir: Implanta\u00e7\u00e3o do OpenStack</p>"},{"location":"roteiros/roteiro3/main/#implantacao-do-openstack","title":"Implanta\u00e7\u00e3o do OpenStack","text":"<ul> <li>Nesse ambiente, o Juju controller deve estar instalado no server1;</li> <li>Para monitorar o status da instala\u00e7\u00e3o, usar o comando: <pre><code>watch -n 2 --color \"juju status --color\"\n</code></pre></li> </ul> <p>Erro de instala\u00e7\u00e3o</p> <p>No caso de problemas durante a instala\u00e7\u00e3o, dependendo da gravidade do problema, \u00e9 mais simples limpar a instala\u00e7\u00e3o:</p> <pre><code>juju kill-controller maas-controller\n</code></pre> <p>E reiniciar desde 1 - Juju Controller.</p> <p>Aviso</p> <p>O roteiro feito, em 2024.2, possui uma vers\u00e3o antiga dos comandos utilizados. Durante essa documenta\u00e7\u00e3o do roteiro, ser\u00e3o apresentados os comandos utilizados na realiza\u00e7\u00e3o do roteiro, e os comandos atualizados estar\u00e3o apresentados em notas intituladas Nova Vers\u00e3o.</p>"},{"location":"roteiros/roteiro3/main/#1-juju-controller","title":"1 - Juju Controller","text":"<p>Apenas se ainda n\u00e3o tiver um Juju Controller, adicionar a tag controller na m\u00e1quina server1:</p> <pre><code>juju bootstrap --bootstrap-series=jammy --constraints tags=controller maas-one maas-controller\n</code></pre>"},{"location":"roteiros/roteiro3/main/#2-modelo-de-deploy","title":"2 - Modelo de deploy","text":"<pre><code>juju add-model --config default-series=jammy openstack\njuju switch maas-controller:openstack\n</code></pre>"},{"location":"roteiros/roteiro3/main/#3-ceph-osd","title":"3 - Ceph OSD","text":"<p>O aplicativo ceph-osd \u00e9 implantado em 3 n\u00f3s usando o charm ceph-osd, com os dispositivos de armazenamento <code>/dev/sda</code> e <code>/dev/sdb</code> configurados para uso em todos os n\u00f3s no arquivo ceph-osd.yaml. \u00c9 utilizada a tag compute, definida previamente nos n\u00f3s via MAAS, com o comando juju deploy especificando as 3 m\u00e1quinas e a configura\u00e7\u00e3o das mesmas. Para fazer o deploy da aplica\u00e7\u00e3o ceph-osd:</p> <pre><code>juju deploy -n 3 --channel reef/stable --config ceph-osd.yaml --constraints tags=compute ceph-osd\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy -n 3 --channel quincy/stable --config ceph-osd.yaml --constraints tags=compute ceph-osd\n</code></pre>"},{"location":"roteiros/roteiro3/main/#4-nova-compute","title":"4 - Nova Compute","text":"<p>O nova-compute, respons\u00e1vel por provisionar inst\u00e2ncias de computa\u00e7\u00e3o no OpenStack, deve ser implantado nos n\u00f3s de computa\u00e7\u00e3o usando os IDs das m\u00e1quinas (0, 1 e 2), pois n\u00e3o h\u00e1 mais n\u00f3s MAAS dispon\u00edveis. Isso implica em compartilhar os mesmos n\u00f3s entre m\u00faltiplos servi\u00e7os. \u00c9 feito o arquivo de configura\u00e7\u00e3o nova-compute.yaml, e para o deploy:</p> <pre><code>juju deploy -n 2 --to 1,2 --channel 2023.2/stable --config nova-compute.yaml nova-compute\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy -n 3 --to 0,1,2 --channel yoga/stable --config nova-compute.yaml nova-compute\n</code></pre>"},{"location":"roteiros/roteiro3/main/#5-mysql-innodb-cluster","title":"5 - MySQL InnoDB Cluster","text":"<p>MySQL InnoDB Cluster requer no m\u00ednimo 3 unidades de banco de dados, e ele deve implantado com o atributo mysql-innodb-cluster, sendo conteinerizados nas m\u00e1quinas 0, 1 e 2, fazendo o deploy como indicado:</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel 8.0/stable mysql-innodb-cluster\n</code></pre>"},{"location":"roteiros/roteiro3/main/#6-vault","title":"6 - Vault","text":"<p>O Vault gerencia os certificados TLS que permitem a comunica\u00e7\u00e3o criptografada entre aplicativos em nuvem, e ele \u00e9 conteinerizado na m\u00e1quina 2:</p> <pre><code>juju deploy --to lxd:0 --channel 1.8/stable vault\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy --to lxd:2 vault --channel 1.8/stable\n</code></pre> <p>Os passos seguintes s\u00e3o de acordo com as intru\u00e7\u00f5es abaixo, com os seus respectivos comandos:</p> <ul> <li>criar inst\u00e2ncia espec\u00edfica do mysql-router com o charme subordinado mysql-router;</li> <li>adicionar rela\u00e7\u00e3o entre inst\u00e2ncia mysql-router e o banco de dados;</li> <li>adicionar rela\u00e7\u00e3o entre inst\u00e2ncia mysql-router e o aplicativo.</li> </ul> <pre><code>juju deploy --channel 8.0/stable mysql-router vault-mysql-router\njuju integrate vault-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate vault-mysql-router:shared-db vault:shared-db\n</code></pre> <p>Para o Vault ser inicializado, desbloqueado e autorizado, s\u00e3o executados os comandos a seguir com os seguintes prop\u00f3sitos:</p> <ul> <li>Instalando o cli do Vault;</li> <li>Configurando o cli;</li> <li>Gerando-os;</li> <li>Remover o selo de 3 teclas (usar esse comando 3 vezes);</li> <li>Confirgurando o token;</li> <li>Gerando um token (tempo dos pr\u00f3ximos passos de 10 minutos);</li> <li>Autorizando.</li> </ul> <pre><code>sudo snap install vault\nexport VAULT_ADDR=\"http://&lt;IP of vault&gt;:8200\"\nvault operator init -key-shares=5 -key-threshold=3\nvault operator unseal &lt;Unseal Key&gt;\nexport VAULT_TOKEN=&lt;Initial Root Token&gt;\nvault token create -ttl=10m\njuju run vault/0 authorize-charm token=&lt;Token generated in the last command&gt;\n</code></pre> Nova Vers\u00e3o <pre><code>juju run vault/leader authorize-charm token=&lt;Token generated in the last command&gt;\n</code></pre> <p>Para o Vault, \u00e9 preciso um certificado de CA autoassinado para que ele possa emitir os certificados para servi\u00e7os de API em nuvem.</p> <pre><code>juju run vault/0 generate-root-ca\n</code></pre> Nova Vers\u00e3o <pre><code>juju run vault/leader generate-root-ca\n</code></pre> <p>Os aplicativos em nuvem s\u00e3o habilitados para TLS por meio da rela\u00e7\u00e3o vault:certificates.</p> <pre><code>juju integrate mysql-innodb-cluster:certificates vault:certificates\n</code></pre>"},{"location":"roteiros/roteiro3/main/#7-neutron-networking","title":"7 - Neutron Networking","text":"<p>A rede de n\u00eautrons \u00e9 implementada com 4 aplica\u00e7\u00f5es, com o arquivo neutron.yaml configurando apenas as aplica\u00e7\u00f5es neutron-api e ovn-chassis, e com isso s\u00e3o feitos os deploys:</p> <ul> <li>ovn-central: no m\u00ednimo 3 unidades -&gt; conteinerizadas nas m\u00e1quinas 0, 1 e 2.</li> <li>neutron-api: conteinerizado na m\u00e1quina 1.</li> <li>neutron-api-plugin-ovn (subordinado): aplica\u00e7\u00e3o charm subordinada.</li> <li>ovn-chassis (subordinado): aplica\u00e7\u00e3o charm subordinada.</li> </ul> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel 23.09/stable ovn-central\njuju deploy --to lxd:1 --channel 2023.2/stable --config neutron.yaml neutron-api\njuju deploy --channel 2023.2/stable neutron-api-plugin-ovn\njuju deploy --channel 23.09/stable --config neutron.yaml ovn-chassis\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel 22.03/stable ovn-central\njuju deploy --to lxd:1 --channel yoga/stable --config neutron.yaml neutron-api\njuju deploy --channel yoga/stable neutron-api-plugin-ovn\njuju deploy --channel 22.03/stable --config neutron.yaml ovn-chassis\n</code></pre> <p>S\u00e3o adicionadas as rela\u00e7\u00f5es necess\u00e1rias:</p> <pre><code>juju integrate neutron-api-plugin-ovn:neutron-plugin neutron-api:neutron-plugin-api-subordinate\njuju integrate neutron-api-plugin-ovn:ovsdb-cms ovn-central:ovsdb-cms\njuju integrate ovn-chassis:ovsdb ovn-central:ovsdb\njuju integrate ovn-chassis:nova-compute nova-compute:neutron-plugin\njuju integrate neutron-api:certificates vault:certificates\njuju integrate neutron-api-plugin-ovn:certificates vault:certificates\njuju integrate ovn-central:certificates vault:certificates\njuju integrate ovn-chassis:certificates vault:certificates\n</code></pre> <p>Conectando a neutron-api ao banco de dados na nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router neutron-api-mysql-router\njuju integrate neutron-api-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate neutron-api-mysql-router:shared-db neutron-api:shared-db\n</code></pre>"},{"location":"roteiros/roteiro3/main/#8-keystone","title":"8 - Keystone","text":"<p>Aplica\u00e7\u00e3o Keystone conteinerizada na m\u00e1quina 0, com deploy: </p> <pre><code>juju deploy --to lxd:0 --channel 2023.2/stable keystone\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy --to lxd:0 --channel yoga/stable keystone\n</code></pre> <p>Conectando o Keystone ao banco de dados na nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router keystone-mysql-router\njuju integrate keystone-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate keystone-mysql-router:shared-db keystone:shared-db\n</code></pre> <p>Adicionando rela\u00e7\u00f5es:</p> <pre><code>juju integrate keystone:identity-service neutron-api:identity-service\njuju integrate keystone:certificates vault:certificates\n</code></pre>"},{"location":"roteiros/roteiro3/main/#9-rabbitmq","title":"9 - RabbitMQ","text":"<p>O aplicativo rabbitmq-server \u00e9 conteinerizado na m\u00e1quina 2 com o charm rabbitmq-server, adicionando em seguida duas rela\u00e7\u00f5es.</p> <pre><code>juju deploy --to lxd:2 --channel 3.9/stable rabbitmq-server\njuju integrate rabbitmq-server:amqp neutron-api:amqp\njuju integrate rabbitmq-server:amqp nova-compute:amqp\n</code></pre>"},{"location":"roteiros/roteiro3/main/#10-nova-cloud-controller","title":"10 - Nova Cloud Controller","text":"<p>Esse aplicativo \u00e9 conteinerizado na m\u00e1quina 2 com o charm nova-cloud-controller. \u00c9 feito o arquivo ncc.yaml, que cont\u00e9m as configura\u00e7\u00f5es, e depois \u00e9 feito o deploy da aplica\u00e7\u00e3o:</p> <pre><code>juju deploy --to lxd:2 --channel 2023.2/stable --config ncc.yaml nova-cloud-controller\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy --to lxd:2 --channel yoga/stable --config ncc.yaml nova-cloud-controller\n</code></pre> <p>Conectando o Nova Cloud Controller ao banco de dados na nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router ncc-mysql-router\njuju integrate ncc-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate ncc-mysql-router:shared-db nova-cloud-controller:shared-db\n</code></pre> <p>Adicionando rela\u00e7\u00f5es:</p> <pre><code>juju integrate nova-cloud-controller:identity-service keystone:identity-service\njuju integrate nova-cloud-controller:amqp rabbitmq-server:amqp\njuju integrate nova-cloud-controller:neutron-api neutron-api:neutron-api\njuju integrate nova-cloud-controller:cloud-compute nova-compute:cloud-compute\njuju integrate nova-cloud-controller:certificates vault:certificates\n</code></pre>"},{"location":"roteiros/roteiro3/main/#11-placement","title":"11 - Placement","text":"<p>A aplica\u00e7\u00e3o Placement \u00e9 conteinerizada na m\u00e1quina 2 com o charm placement.</p> <pre><code>juju deploy --to lxd:1 --channel 2023.2/stable placement\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy --to lxd:2 --channel yoga/stable placement\n</code></pre> <p>Conectando o Placement ao banco de dados na nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router placement-mysql-router\njuju integrate placement-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate placement-mysql-router:shared-db placement:shared-db\n</code></pre> <p>Adicionando rela\u00e7\u00f5es:</p> <pre><code>juju integrate placement:identity-service keystone:identity-service\njuju integrate placement:placement nova-cloud-controller:placement\njuju integrate placement:certificates vault:certificates\n</code></pre>"},{"location":"roteiros/roteiro3/main/#12-horizon-openstack-dashboard","title":"12 - Horizon - OpenStack Dashboard","text":"<p>O Horizon \u00e9 conteinerizado na m\u00e1quina 2 com o charm openstack-dashboard.</p> <pre><code>juju deploy --to lxd:2 --channel 2023.2/stable openstack-dashboard\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy --to lxd:2 --channel yoga/stable openstack-dashboard\n</code></pre> <p>Conectando o Horizon ao banco de dados na nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router dashboard-mysql-router\njuju integrate dashboard-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate dashboard-mysql-router:shared-db openstack-dashboard:shared-db\n</code></pre> <p>Adicionando rela\u00e7\u00f5es:</p> <pre><code>juju integrate openstack-dashboard:identity-service keystone:identity-service\njuju integrate openstack-dashboard:certificates vault:certificates\n</code></pre>"},{"location":"roteiros/roteiro3/main/#13-glance","title":"13 - Glance","text":"<p>A aplica\u00e7\u00e3o Glance \u00e9 conteinerizada na m\u00e1quina 2 com o charm glance.</p> <pre><code>juju deploy --to lxd:2 --channel 2023.2/stable glance\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy --to lxd:2 --channel yoga/stable glance\n</code></pre> <p>Conectando o Glance ao banco de dados na nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router glance-mysql-router\njuju integrate glance-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate glance-mysql-router:shared-db glance:shared-db\n</code></pre> <p>Adicionando rela\u00e7\u00f5es:</p> <pre><code>juju integrate glance:image-service nova-cloud-controller:image-service\njuju integrate glance:image-service nova-compute:image-service\njuju integrate glance:identity-service keystone:identity-service\njuju integrate glance:certificates vault:certificates\n</code></pre>"},{"location":"roteiros/roteiro3/main/#14-ceph-monitor","title":"14 - Ceph Monitor","text":"<p>O Ceph Monitor \u00e9 conteinerizado nas m\u00e1quinas 0, 1 e 2 com o charm ceph-mon. Primeiro \u00e9 feito o arquivo ceph-mon.yaml, que cont\u00e9m as configura\u00e7\u00f5es, e depois \u00e9 feito o deploy da aplica\u00e7\u00e3o:</p> <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel reef/stable --config ceph-mon.yaml ceph-mon\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy -n 3 --to lxd:0,lxd:1,lxd:2 --channel quincy/stable --config ceph-mon.yaml ceph-mon\n</code></pre> <p>Adicionando rela\u00e7\u00f5es:</p> <pre><code>juju integrate ceph-mon:osd ceph-osd:mon\njuju integrate ceph-mon:client nova-compute:ceph\njuju integrate ceph-mon:client glance:ceph\n</code></pre>"},{"location":"roteiros/roteiro3/main/#15-cinder","title":"15 - Cinder","text":"<p>O Cinder \u00e9 conteinerizado na m\u00e1quina 1 com o charm cinder. \u00c9 criado o arquivo de configura\u00e7\u00e3o cinder.yaml, e depois \u00e9 feito o deploy da aplica\u00e7\u00e3o:</p> <pre><code>juju deploy --to lxd:1 --channel 2023.2/stable --config cinder.yaml cinder\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy --to lxd:1 --channel yoga/stable --config cinder.yaml cinder\n</code></pre> <p>Conectando o Cinder ao banco de dados na nuvem:</p> <pre><code>juju deploy --channel 8.0/stable mysql-router cinder-mysql-router\njuju integrate cinder-mysql-router:db-router mysql-innodb-cluster:db-router\njuju integrate cinder-mysql-router:shared-db cinder:shared-db\n</code></pre> <p>Adicionando rela\u00e7\u00f5es:</p> <pre><code>juju integrate cinder:cinder-volume-service nova-cloud-controller:cinder-volume-service\njuju integrate cinder:identity-service keystone:identity-service\njuju integrate cinder:amqp rabbitmq-server:amqp\njuju integrate cinder:image-service glance:image-service\njuju integrate cinder:certificates vault:certificates\n</code></pre> <p>A rela\u00e7\u00e3o glance:image-service permite que o Cinder consuma a API do Glance e, como o Glance, o Cinder usa o ceph como backend de armazenamento, implementado com o comando subordinado a seguir:</p> <pre><code>juju deploy --channel 2023.2/stable cinder-ceph\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy --channel yoga/stable cinder-ceph\n</code></pre> <p>Adicionando rela\u00e7\u00f5es:</p> <pre><code>juju integrate cinder-ceph:storage-backend cinder:storage-backend\njuju integrate cinder-ceph:ceph ceph-mon:client\njuju integrate cinder-ceph:ceph-access nova-compute:ceph-access\n</code></pre>"},{"location":"roteiros/roteiro3/main/#16-ceph-rados-gateway","title":"16 - Ceph RADOS Gateway","text":"<p>O Ceph RADOS Gateway, implantado para oferecer um gateway HTTP compat\u00edvel com S3 e Swift, \u00e9 conteinerizado na m\u00e1quina 0 com o charm ceph-radosgw.</p> <pre><code>juju deploy --to lxd:0 --channel reef/stable ceph-radosgw\n</code></pre> Nova Vers\u00e3o <pre><code>juju deploy --to lxd:0 --channel quincy/stable ceph-radosgw\n</code></pre> <p>Adicionando rela\u00e7\u00e3o:</p> <pre><code>juju integrate ceph-radosgw:mon ceph-mon:radosgw\n</code></pre>"},{"location":"roteiros/roteiro3/main/#17-ceph-osd-integration","title":"17 - Ceph-OSD Integration","text":"<p>Com todos os comandos anteriores corretamente realizados, \u00e9 preciso executar a configura\u00e7\u00e3o do charm ceph-osd para usar o dispositivo de bloco <code>/dev/sdb</code> como armazenamento para os OSDs do Ceph.</p> <pre><code>juju config ceph-osd osd-devices='/dev/sdb'\n</code></pre>"},{"location":"roteiros/roteiro3/main/#configurando-o-openstack","title":"Configurando o Openstack","text":"<ul> <li>Documenta\u00e7\u00e3o da refer\u00eancia oficial: https://docs.openstack.org/project-deploy-guide/charm-deployment-guide/latest/configure-openstack.html</li> </ul> <p>Ser\u00e3o configurados os servi\u00e7os que controlam as VMs (Nova), os volumes de disco (Cinder), e a estrutura de rede virtual (Neutron). E para isso, ser\u00e3o seguidos os seguintes passos:</p>"},{"location":"roteiros/roteiro3/main/#passo-1-autenticacao","title":"Passo 1: Autentica\u00e7\u00e3o","text":"<p>Criando o arquivo <code>openrc</code> com as credenciais de acesso ao OpenStack.</p> <ul> <li> <p>Obtendo o endere\u00e7o IP: <pre><code>juju status --format=yaml openstack-dashboard | grep public-address | awk '{print $2}' | head -1\n</code></pre></p> </li> <li> <p>Obtendo a senha: <pre><code>juju run keystone/0 get-admin-password\n</code></pre></p> </li> </ul>"},{"location":"roteiros/roteiro3/main/#passo-2-horizon","title":"Passo 2: Horizon","text":"<p>Acessando o dashboard Horizon como administrador e mantendo ele aberto durante todo o setup do openstack para visualizar as mudan\u00e7as que est\u00e3o sendo feitas.</p> <ul> <li>Horizon_URL: https://#IP_obtido#/horizon</li> <li>User_Name: admin</li> <li>Password: #Senha_obtida#</li> <li>Domain_Name: admin_domain</li> </ul> Tarefa-1 <p>De um print das Telas abaixo:</p> <pre><code>1. Do Status do JUJU\n2. Do Dashboard do MAAS com as m\u00e1quinas.\n3. Da aba compute overview no OpenStack Dashboard.\n4. Da aba compute instances no OpenStack Dashboard.\n5. Da aba network topology no OpenStack Dashboard.\n</code></pre> <p></p> <p>Figura 2 - Comando \u201cjuju status\u201d no terminal.</p> <p></p> <p>Figura 3 - Dashboard do MAAS com as m\u00e1quinas.</p> <p></p> <p>Figura 4 - Aba Compute Overview no OpenStack.</p> <p></p> <p>Figura 5 - Aba Compute Instances no OpenStack.</p> <p></p> <p>Figura 6 - Aba Network Topoplogy no OpenStack.</p>"},{"location":"roteiros/roteiro3/main/#passo-3-imagens","title":"Passo 3: Imagens","text":"<ul> <li> <p>Instalando o cliente do Openstack no main via snap: <pre><code>sudo snap install openstackclients\n</code></pre></p> </li> <li> <p>Carregando as credenciais em openrc: <pre><code>source ~/openstack-bundles/stable/openstack-base/openrc\n</code></pre></p> </li> <li> <p>Verificando os servi\u00e7os dispon\u00edveis no Openstack: <pre><code>openstack service list\n</code></pre></p> </li> <li> <p>Fazendo ajustes na rede: <pre><code>juju config neutron-api enable-ml2-dns=\"true\"\njuju config neutron-api-plugin-ovn dns-servers=\"172.20.129.131\"\n</code></pre></p> </li> <li> <p>Procurando e importando a imagem do Ubuntu Jammy para o Glance: <pre><code>curl http://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-\namd64.img \\\n    --output ~/cloud-images/jammy-amd64.img\n\nopenstack image create --public --container-format bare \\\n    --disk-format qcow2 --file ~/cloud-images/jammy-amd64.img \\\n    jammy-amd64\n</code></pre></p> </li> </ul> <p>N\u00e3o esquecer!</p> <p>Fazer o clone do reposit\u00f3rio <code>openstack-bundles</code> caso ainda n\u00e3o tenha feito. <pre><code>git clone https://github.com/openstack-charmers/openstack-bundles\n</code></pre></p>"},{"location":"roteiros/roteiro3/main/#passo-4-rede-externa","title":"Passo 4: Rede Externa","text":"<p>Configurando uma rede externa para conectar as VMs \u00e0 rede f\u00edsica, usando uma faixa de aloca\u00e7\u00e3o entre 172.16.7.0 e 172.16.8.255:</p> <pre><code>source ~/openstack-bundles/stable/openstack-base/openrc\n\nopenstack network create --external \\\n    --provider-network-type flat --provider-physical-network physnet1 \\\n    ext_net\n\nopenstack subnet create --network ext_net --no-dhcp \\\n    --gateway 172.16.0.1 --subnet-range 172.16.0.0/20 \\\n    --allocation-pool start=172.16.7.0,end=172.16.8.255 \\\n    ext_subnet\n</code></pre>"},{"location":"roteiros/roteiro3/main/#passo-5-rede-interna-e-roteador","title":"Passo 5: Rede Interna e Roteador","text":"<p>Configurando uma rede interna para conectar as VMs \u00e0 rede externa, usando a subnet 192.169.0.0/24 e sem DNS:</p> <pre><code>openstack network create int_net\n\nopenstack subnet create --network int_net \\\n   --gateway 192.169.0.1 --subnet-range 192.169.0.0/24 \\\n   --allocation-pool start=192.169.0.10,end=192.169.0.200 \\\n   int_subnet\n</code></pre> <p>Configurando o roteador:</p> <pre><code>openstack router create provider-router\nopenstack router set --external-gateway ext_net provider-router\nopenstack router add subnet provider-router int_subnet\n</code></pre>"},{"location":"roteiros/roteiro3/main/#passo-6-flavors","title":"Passo 6: Flavors","text":"<ul> <li>Criando os flavors (instance type) - SEM ephemeral disk para as VMs:</li> </ul> Flavor Name vCPUs RAM (GB) Disk <code>m1.tiny</code> 1 1 20 <code>m1.small</code> 1 2 20 <code>m1.medium</code> 2 4 20 <code>m1.large</code> 4 8 20 <p>Tabela 1 - Flavors criados e suas respectivas configura\u00e7\u00f5es. </p> <p>Comandos: <pre><code>openstack flavor create --ram 1000 --vcpus 1 --disk 20 m1.tiny\nopenstack flavor create --ram 2000 --vcpus 1 --disk 20 m1.small\nopenstack flavor create --ram 4000 --vcpus 2 --disk 20 m1.medium\nopenstack flavor create --ram 8000 --vcpus 4 --disk 20 m1.large\n</code></pre></p>"},{"location":"roteiros/roteiro3/main/#passo-7-conexao","title":"Passo 7: Conex\u00e3o","text":"<p>O key-pair da m\u00e1quina onde est\u00e1 o MaaS j\u00e1 existe e \u00e9 importado:</p> <pre><code>openstack keypair create --public-key ~/.ssh/id_rsa.pub open_key\n</code></pre> <ul> <li>Via o dashboard Horizon, como administrador, adicionando a libera\u00e7\u00e3o do SSH e ALL ICMP no security group default:</li> </ul> <pre><code>for i in $(openstack security group list | awk '/default/{ print $2 }'); do\n   openstack security group rule create $i --protocol icmp --remote-ip \n0.0.0.0/0;    \n   openstack security group rule create $i --protocol tcp --remote-ip \n0.0.0.0/0 --dst-port 22; \ndone\n</code></pre>"},{"location":"roteiros/roteiro3/main/#passo-8-instancia","title":"Passo 8: Inst\u00e2ncia","text":"<ul> <li> <p>Disparando uma inst\u00e2ncia m1.tiny com o nome client e sem Novo Volume: <pre><code>openstack server create --image jammy-amd64 --flavor m1.tiny \\\n   --key-name open_key --network int_net \\\n   client\n</code></pre></p> </li> <li> <p>Alocando um floating IP para a inst\u00e2ncia: <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net)\n\nopenstack server add floating ip client $FLOATING_IP\n</code></pre></p> </li> <li> <p>Testando a conex\u00e3o SSH: <pre><code>ssh ubuntu@$FLOATING_IP\n</code></pre></p> </li> </ul> Tarefa-2 <p>De um print das Telas abaixo:</p> <pre><code>1. Do Dashboard do MAAS com as m\u00e1quinas.\n2. Da aba compute overview no OpenStack.\n3. Da aba compute instances no OpenStack.\n4. Da aba network topology no OpenStack.\n</code></pre> <p>Enumere as diferencas encontradas entre os prints das telas na Tarefa 1 e na Tarefa 2.</p> <p>Explique como cada recurso foi criado.</p> <p></p> <p>Figura 7 - Dashboard do MAAS com as m\u00e1quinas.</p> <p></p> <p>Figura 8 - Aba Compute Overview no OpenStack.</p> <p></p> <p>Figura 9 - Aba Compute Instances no OpenStack.</p> <p></p> <p>Figura 10 - Aba Network Topoplogy no OpenStack.</p> <p>Enumere as diferencas encontradas entre os prints das telas na Tarefa 1 e na Tarefa 2.</p> <ul> <li>Compute overview: percebe-se que o n\u00famero de Inst\u00e2ncias, VCPUs, Floating IPs, security groups e Routers atualizou para 1, pois eles foram criados. Al\u00e9m disso, a mem\u00f3ria RAM agora est\u00e1 em 1GB, temos 6 rules dos security groups e a networks e a port atualizaram os valores, pois agora estamos utilizando inst\u00e2ncias; </li> <li>Compute instances: criada a inst\u00e2ncia \u201ccliente\u201d; </li> <li>Network topology: tem network, uma vez que foi criada a subnet e o roteador.</li> </ul> <p>Explique como cada recurso foi criado.</p> <p>Ap\u00f3s importar as chaves de valida\u00e7\u00e3o e imagem, e configurado a rede externa: </p> <ul> <li> <p>Rede interna e roteador: usar uma s\u00e9rie de comandos para criar rede interna, subrede e roteador, colocando os devidos valores. <pre><code>openstack network create int_net \nopenstack subnet create --network int_net --gateway 192.168.0.1 --subnet-range 192.168.0.0/24 --allocation-pool start=192.168.0.10,end=192.168.0.200 int_subnet \nopenstack router create provider-router \nopenstack router set --external-gateway ext_net provider-router \nopenstack router add subnet provider-router int_subnet\n</code></pre></p> </li> <li> <p>Security Groups Rules: usar um comando  <pre><code>openstack security group rule create\n</code></pre></p> </li> <li> <p>Inst\u00e2ncia: usar o comando a seguir passando a devida imagem, chave de valida\u00e7\u00e3o e rede.  <pre><code>openstack server create --image jammy-amd64 --flavor m1.small --key-name mykey --network int_net client\n</code></pre></p> </li> <li> <p>Endere\u00e7o de IP Flutuante: comandos abaixo, passando o nome do servidor criado anteriormente. <pre><code>FLOATING_IP=$(openstack floating ip create -f value -c floating_ip_address ext_net) openstack server add floating ip client $FLOATING_IP\n</code></pre></p> </li> </ul>"},{"location":"roteiros/roteiro3/main/#escalando-os-nos","title":"Escalando os n\u00f3s","text":"<p>No OpenStack, escalar os n\u00f3s de configura\u00e7\u00e3o \u00e9 essencial para melhorar diversos aspectos dos servi\u00e7os em um ambiente de nuvem. </p> <p>Alguns benef\u00edcios da escala dos n\u00f3s de configura\u00e7\u00e3o:</p> <ul> <li>Aumento de capacidade de processamento;</li> <li>Alta disponibilidade e toler\u00e2ncia a falhas;</li> <li>Melhoria de desempenho e lat\u00eancia reduzida;</li> <li>Escalabilidade horizontal.</li> </ul> Info <p>Para mais detalhes sobre o escalonamento de n\u00f3s no OpenStack, visitar o site da disciplina: escalonando os n\u00f3s</p> <p>Adicionando o n\u00f3 reserva ao openstack no cluster como n\u00f3 de computing e block storage.</p> <ul> <li> <p>Fazer o release da m\u00e1quina que est\u00e1 ALLOCATED no Dashboard do MaaS.</p> </li> <li> <p>Instalando o hypervisor, realizando o deploy na m\u00e1quina: <pre><code>juju add-unit nova-compute\n</code></pre></p> </li> <li> <p>Ao anotar o n\u00famero da m\u00e1quina adicionada no status, \u00e9 instalado o block storage: <pre><code>juju add-unit --to &lt;machine-id&gt; ceph-osd\n</code></pre></p> </li> </ul> Tarefa-3 <p>Fa\u00e7a um desenho de como \u00e9 a sua arquitetura de rede, desde a sua conex\u00e3o com o Insper at\u00e9 a inst\u00e2ncia alocada.</p> <p></p> <p>Figura 11 - Desenho da arquitetura de rede, da conex\u00e3o com o Insper at\u00e9 a inst\u00e2ncia alocada.</p>"},{"location":"roteiros/roteiro3/main/#app-uso-da-infraestrutura","title":"App - Uso da infraestrutura","text":"<p>Levantar 4 inst\u00e2ncias em m\u00e1quinas virtuais do OpenStack: 2 inst\u00e2ncias com a API do projeto, 1 inst\u00e2ncia com banco de dados e 1 inst\u00e2ncia com LoadBalancer (Nginx). Configurando-as como indicado a seguir:</p> <p></p> <p>Figura 12 - Topologia a ser configurada. Fonte: instru\u00e7\u00f5es da disciplina.</p> Tarefa-4 <ol> <li>Escreva um relat\u00f3rio dos passos utilizados. </li> <li>Anexe fotos e/ou diagramas contendo: arquitetura de rede da sua infraestrutura dentro do Dashboard do Openstack.</li> <li>Lista de VMs utilizadas com nome e IPs alocados </li> <li>Print do Dashboard do Wordpress conectado via m\u00e1quina Nginx/LB. </li> <li>4 Prints, cada um demonstrando em qual server  (m\u00e1quina f\u00edsica) cada instancia foi alocado pelo OpenStack.</li> </ol> <p>O relat\u00f3rio est\u00e1 presente na aba Relat\u00f3rio - Tarefa 4.</p>"},{"location":"roteiros/roteiro3/main/#conclusao","title":"Conclus\u00e3o","text":"<p>Este roteiro apresentou, de forma pr\u00e1tica e detalhada, o processo de implanta\u00e7\u00e3o de uma nuvem privada utilizando o OpenStack em conjunto com ferramentas como MAAS, Juju e charms espec\u00edficos. </p> <p>Foram abordadas todas as etapas necess\u00e1rias para configurar a infraestrutura, desde a cria\u00e7\u00e3o do controller at\u00e9 a implanta\u00e7\u00e3o dos principais servi\u00e7os OpenStack \u2014 incluindo redes (Neutron com SDN/OVN), armazenamento (Ceph e Cinder), autentica\u00e7\u00e3o (Keystone), dashboard (Horizon), imagens (Glance), computa\u00e7\u00e3o (Nova) e banco de dados (MySQL). Al\u00e9m disso, destacou-se a import\u00e2ncia do Vault na gest\u00e3o de certificados e seguran\u00e7a. </p> <p>Ao longo do roteiro, tamb\u00e9m foram apresentadas atualiza\u00e7\u00f5es dos comandos usados, refletindo vers\u00f5es mais recentes dos componentes. O conte\u00fado permite n\u00e3o apenas a compreens\u00e3o dos conceitos fundamentais de nuvem privada, mas tamb\u00e9m oferece uma vis\u00e3o aplicada sobre redes virtuais baseadas em SDN.</p>"},{"location":"roteiros/roteiro3/relatorio/","title":"Relat\u00f3rio","text":""},{"location":"roteiros/roteiro3/relatorio/#uso-da-infraestrutura","title":"Uso da Infraestrutura","text":"<p>Com o objetivo de levantar uma aplica\u00e7\u00e3o com Nginx, WordPress e MySQL, foram realizados uma s\u00e9rie de passos e comandos que ser\u00e3o explicados ao longo do relat\u00f3rio. </p>"},{"location":"roteiros/roteiro3/relatorio/#instancias","title":"Inst\u00e2ncias","text":"<p>Primeiramente, entrou-se no Dashboard do Openstack para criar as 4 inst\u00e2ncias necess\u00e1rias na aba \"Compute\" --&gt; \"Instances\" e para configurar as inst\u00e2ncias selecionar \"Launch  Instance\". Para a configura\u00e7\u00e3o, \u00e9 necess\u00e1rio inserir alguns dados para cada uma: </p> <ul> <li>Nome da inst\u00e2ncia (aba Details) </li> <li>Selecionar source Image e alocar a imagem \"jammy-amd64\" (aba Source) </li> <li>Alocar o flavor m1.tiny (aba Flavor) </li> <li>Alocar a rede interna int_net (aba Network) </li> <li>Apenas para a inst\u00e2ncia nginx: tamb\u00e9m \u00e9 preciso alocar a rede externa ext_net </li> </ul>"},{"location":"roteiros/roteiro3/relatorio/#mysql","title":"MySQL","text":"<p>Ap\u00f3s criar a inst\u00e2ncia onde o servidor mysql ir\u00e1 rodar, ela foi acessada via ssh, e para isso foi adicionado um IP Flutuante. Ap\u00f3s acess\u00e1-la, foi instalado o mysql (mysql server), via apt e, feito isso, os arquivos de configura\u00e7\u00e3o do mysql foram alterados para que o servidor sql fosse acess\u00edvel de qualquer ponto da rede. Ap\u00f3s realizada as altera\u00e7\u00f5es, o servi\u00e7o foi reiniciado usando o comando do systemcl. Finalizando ap\u00f3s confirmar que o servidor estava acess\u00edvel externamente, foi adicionado um novo banco de dados chamado \u201cWordPress\u201d onde um novo usu\u00e1rio \u201ccloud\u201d foi dado permiss\u00e3o para realizar qualquer opera\u00e7\u00e3o nessa data-base atrav\u00e9s de qualquer host (\u201c%\u201d), terminando assim as configura\u00e7\u00f5es do MySQL e removendo o IP flutuante da inst\u00e2ncia, isolando ele \u00e0 rede interna. </p>"},{"location":"roteiros/roteiro3/relatorio/#nginx","title":"Nginx","text":"<p>Depois de criar a inst\u00e2ncia onde o servidor nginx ir\u00e1 rodar, ela foi acessada via ssh, e como neste caso a inst\u00e2ncia possui um IP externo, n\u00e3o foi necess\u00e1rio adicionar um IP flutuante para acessar a inst\u00e2ncia. Foi instalado o nginx via apt e ap\u00f3s confirmar que o servi\u00e7o estava funcionado e acess\u00edvel remotamente foi adicionado na configura\u00e7\u00e3o padr\u00e3o de sites o m\u00f3dulo \u201cUpstream\" e foi aplicada a regra para redirecionar qualquer chamada para os servidores \"Apache Web\u201d das instancias \u201cWordPress\u201d.</p>"},{"location":"roteiros/roteiro3/relatorio/#wordpress","title":"WordPress","text":"<p>Para a instala\u00e7\u00e3o e configura\u00e7\u00e3o das inst\u00e2ncias do \u201cWordPress\u201d foi adicionado um IP Flutuante em cada inst\u00e2ncia para conectar via ssh e ap\u00f3s o acesso foram seguidos os passos do tutorial no link: https://ubuntu.com/tutorials/install-and-configure-wordpress#1-overview, instalando as depend\u00eancias e o \u201cWordpress\u201d, configurando o apache e, por fim, configurando o worpress para conectar \u00e0 base de dados do mysql remoto em vez de local. Conclu\u00edda a instala\u00e7\u00e3o e configura\u00e7\u00e3o do \u201cWordpress\u201d em cada inst\u00e2ncia, os IPs Flutuantes foram removidos.</p>"},{"location":"roteiros/roteiro3/relatorio/#rede-e-maquinas-funcionando","title":"Rede e M\u00e1quinas funcionando","text":"<p>Figura 1 - Desenho da arquitetura de rede, da conex\u00e3o com o Insper at\u00e9 as inst\u00e2ncias alocadas.</p> <p></p> <p>Figura 2 - Arquitetura de rede da infraestrutura no Dashboard do Openstack.</p> <p></p> <p>Figura 3 - Print do Dashboard do Wordpress conectado via m\u00e1quina Nginx/LB. </p> <p></p> <p>Figura 4 - Print dos NATs existentes, mostrando o NAT do Nginx.</p> <p></p> <p>Figura 5 - Print mostrando qual m\u00e1quina a inst\u00e2ncia mysql foi alocada pelo OpenStack.</p> <p></p> <p>Figura 6 - Print mostrando qual m\u00e1quina a inst\u00e2ncia wordpress-1 foi alocada pelo OpenStack.</p> <p></p> <p>Figura 7 - Print mostrando qual m\u00e1quina a inst\u00e2ncia wordpress-2 foi alocada pelo OpenStack.</p> <p></p> <p>Figura 8 - Print mostrando qual m\u00e1quina a inst\u00e2ncia nginx foi alocada pelo OpenStack.</p> <p>Para uma maior facilidade de visualiza\u00e7\u00e3o das VMs, com seus nomes, m\u00e1quinas e IPs, consultar a Tabela 1 a seguir: </p> Nome M\u00e1quina IP alocado <code>MySQL</code> mysql Server 5 192.169.0.188 <code>WordPress</code> wordpress-1 Server 5 192.169.0.93 - 172.16.7.26 <code>WordPress</code> wordpress-2 Server 2 192.169.0.79 - 172.16.8.36 <code>Nginx</code> ngynx Server 5 192.169.0.108 - 172.16.8.133 <p>Tabela 1 - VMs utilizadas, com seus nomes e IPs. </p>"}]}